{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBeyes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyDmfMcpboW+Pom2cMCHuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nils19/clickbaitanalyzer/blob/main/NaiveBeyes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clickbait detector using Naive Bayes Classifier\n",
        "\n Heavily borrowed from Kaggle username:AMAN ANAND and adjusted for use on upworthy data",
        "This kernel focuses on classifying News headlines into clickbaits and non-clickbaits.\n",
        "\n",
        "The clickbaits are labelled as **1** and non-clickbaits as **0**.\n",
        "The headlines are collected from different news sites.\n",
        "\n",
        "The dataset consists of 32000 headlines of which 50% are clickbaits and the other 50% are non-clickbait.\n",
        "\n",
        "I have used a *Multinomial Naive Bayes* classification algorithm for text classification of the given dataset. \n",
        "\n",
        "# Importing different tools and libraries\n",
        "\n",
        "The main libraries used are *Numpy*, *Pandas*, *NLTK*(Natural language toolkit) and *Scikit-learn*.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k6-BQiyg_P-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "import string as s\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "2BWGRMIF_URH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v5JDkqZj_N3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Splitting into Train and Test sets\n",
        "\n",
        "The dataset is splitted into training and testing sets. The percentage of training data is 75% and testing data is 25%.\n"
      ],
      "metadata": {
        "id": "OG-SHF2q_myR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset\n",
        "cb_data= pd.read_csv('/content/clickbait_data.csv')\n",
        "cb_data.head()\n",
        "datahackdata = pd.read_csv('/content/upworthy-archive.csv')\n",
        "datahackdata.head()\n",
        "x=cb_data.headline\n",
        "y=cb_data.clickbait\n",
        "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2395,random_state=2)\n",
        "\n",
        "# winners_df = datahackdata[datahackdata[\"winner\"] == True]\n",
        "# print(len(winners_df))\n",
        "# created_at = datahackdata[datahackdata[\"created_at\"]]\n",
        "# [datahackdata[\"winner\"] == True]\n",
        "# test_x = winners_df.iloc[:12]\n",
        "# print(winners_df)\n",
        "# print(winners_df)\n",
        "test_x  = datahackdata[datahackdata[\"winner\"] == True]\n",
        "print(len(test_x))\n",
        "\n",
        "\n",
        "\n",
        "# Analyzing Train and Test Data\n",
        "# print(\"No. of elements in training set\")\n",
        "# print(train_x.size)\n",
        "# print(\"No. of elements in testing set\")\n",
        "# print(test_x.size)\n",
        "\n",
        "train_x.head(10)\n",
        "train_y.head(10)\n",
        "test_x.head()\n",
        "test_y.head()\n",
        "# Tokenization of Data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_8QIT435_rYr",
        "outputId": "7ef460fd-7354-47ec-d059-3ca1e597e030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c5e86f614d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcb_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/clickbait_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcb_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdatahackdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/upworthy-archive.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdatahackdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 136835"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'. "
      ],
      "metadata": {
        "id": "jnWxNkiGBCyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is tokenized i.e. split into tokens which are the smallest or minimal meaningful units. The data is split into words."
      ],
      "metadata": {
        "id": "bhLqJFaGBE9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization of Data\n",
        "def tokenization(text):\n",
        "    lst=str(text).split()\n",
        "    return lst\n",
        "train_x=train_x.apply(tokenization)\n",
        "test_x=test_x.apply(tokenization)"
      ],
      "metadata": {
        "id": "ewSGs3EBAhfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'."
      ],
      "metadata": {
        "id": "jS0aDur3BQO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercasing(lst):\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        i=i.lower()\n",
        "        new_lst.append(i)\n",
        "    return new_lst\n",
        "train_x=train_x.apply(lowercasing)\n",
        "test_x=test_x.apply(lowercasing)  "
      ],
      "metadata": {
        "id": "I4FI9JNkBMKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The punctuations are removed to increase the efficiency of the model. They are irrelevant because they provide no added information.\n"
      ],
      "metadata": {
        "id": "4OMnPVV-BW0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(lst):\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        for j in s.punctuation:\n",
        "            i=i.replace(j,'')\n",
        "        new_lst.append(i)\n",
        "    return new_lst\n",
        "train_x=train_x.apply(remove_punctuations)\n",
        "test_x=test_x.apply(remove_punctuations) "
      ],
      "metadata": {
        "id": "jpTGmTEdBZz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Numbers\n",
        "def remove_numbers(lst):\n",
        "    nodig_lst=[]\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        for j in s.digits:    \n",
        "            i=i.replace(j,'')\n",
        "        nodig_lst.append(i)\n",
        "    for i in nodig_lst:\n",
        "        if i!='':\n",
        "            new_lst.append(i)\n",
        "    return new_lst\n",
        "train_x=train_x.apply(remove_numbers)\n",
        "test_x=test_x.apply(remove_numbers)"
      ],
      "metadata": {
        "id": "K8tE6mlCBkSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Stopwords\n",
        "nltk.download('stopwords')\n",
        "print(\"All stopwords of English language \")\n",
        "\", \".join(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(lst):\n",
        "    stop=stopwords.words('english')\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        if i not in stop:\n",
        "            new_lst.append(i)\n",
        "    return new_lst\n",
        "\n",
        "train_x=train_x.apply(remove_stopwords)\n",
        "test_x=test_x.apply(remove_stopwords)  "
      ],
      "metadata": {
        "id": "hEFxPOkSBrUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing extra spaces\n",
        "def remove_spaces(lst):\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        i=i.strip()\n",
        "        new_lst.append(i)\n",
        "    return new_lst\n",
        "train_x=train_x.apply(remove_spaces)\n",
        "test_x=test_x.apply(remove_spaces)"
      ],
      "metadata": {
        "id": "rA-V-LXpByE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing the data i.e. after removing punctuation, stopwords, spaces and numbers."
      ],
      "metadata": {
        "id": "mZ8o1HT2B4g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing data after preprocessing\n",
        "train_x.head()\n",
        "test_x.head()"
      ],
      "metadata": {
        "id": "WPAMPRRdB81z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Lemmatization\n",
        "\n",
        "Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It involves the morphological analysis of words.\n",
        "\n",
        "In lemmatization we find the root word or base form of the word rather than just clipping some characters from the end e.g. *is, are, am* are all converted to its base form *be* in Lemmatization\n",
        "\n",
        "Here lemmatization is done using NLTK library."
      ],
      "metadata": {
        "id": "kMHFTKJRB9_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
        "def lemmatzation(lst):\n",
        "    new_lst=[]\n",
        "    for i in lst:\n",
        "        i=lemmatizer.lemmatize(i)\n",
        "        new_lst.append(i)\n",
        "    return new_lst\n",
        "train_x=train_x.apply(lemmatzation)\n",
        "test_x=test_x.apply(lemmatzation)\n",
        "train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\n",
        "test_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))\n",
        "freq_dist={}\n",
        "for i in train_x.head(20):\n",
        "    x=i.split()\n",
        "    for j in x:\n",
        "        if j not in freq_dist.keys():\n",
        "            freq_dist[j]=1\n",
        "        else:\n",
        "            freq_dist[j]+=1\n",
        "freq_dist"
      ],
      "metadata": {
        "id": "ZghPpuUgCDsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is used to convert the text into features."
      ],
      "metadata": {
        "id": "rCfxFfn8CP5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "train_1=tfidf.fit_transform(train_x)\n",
        "test_1=tfidf.transform(test_x)\n",
        "print(\"Number of features extracted\")\n",
        "print(len(tfidf.get_feature_names()))\n",
        "print()\n",
        "print(\"The 100 features extracted from TF-IDF \")\n",
        "print(tfidf.get_feature_names()[:100])\n",
        "print(\"Shape of train set\",train_1.shape)\n",
        "print(\"Shape of test set\",test_1.shape)\n",
        "train_arr=train_1.toarray()\n",
        "test_arr=test_1.toarray()"
      ],
      "metadata": {
        "id": "ScNxFJLWCVC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Naive Bayes Classifier\n",
        "NB_MN=MultinomialNB()\n"
      ],
      "metadata": {
        "id": "tEZek4iBCb_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "NB_MN.fit(train_arr,train_y)\n",
        "pred=NB_MN.predict(test_arr)\n",
        "count = 0\n",
        "nonclick = 0\n",
        "\n",
        "for i in pred.tolist():\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "  else:\n",
        "    nonclick +=1\n",
        "print(count)\n",
        "print(nonclick)\n",
        "  \n",
        "print('first 20 predicted labels: ',pred.tolist())"
      ],
      "metadata": {
        "id": "l2EJ20LcChP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Accuracy and F1 score of the model are printed to evaluate the model for text classification."
      ],
      "metadata": {
        "id": "xLd2yWrJCn2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of Result\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "print(\"F1 score of the model\")\n",
        "# print(f1_score(test_y,pred))\n",
        "print(\"Accuracy of the model\")\n",
        "print(len(test_y))\n",
        "print(len(test_x))\n",
        "print(pred)\n",
        "print(accuracy_score(test_y,pred))\n",
        "print(\"Accuracy of the model in percentage\")\n",
        "print(accuracy_score(test_y,pred)*100,\"%\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(test_y,pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(test_y,pred))"
      ],
      "metadata": {
        "id": "wx8AcahkCogy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## A second model for TF-IDF with different n-grams and fixed feature size\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer(ngram_range=(1,3),max_features=6500)\n",
        "train_2=tfidf.fit_transform(train_x)\n",
        "test_2=tfidf.transform(test_x)\n",
        "\n",
        "NB_MN.fit(train_2.toarray(),train_y)\n",
        "pred2=NB_MN.predict(test_2.toarray())\n",
        "\n",
        "print(\"Accuracy of the model in percentage\")\n",
        "print(accuracy_score(test_y,pred2)*100,\"%\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(test_y,pred2))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(test_y,pred2))"
      ],
      "metadata": {
        "id": "CWUCM3oECyTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datahackdata = pd.read_csv('/content/upworthy-archive.csv')\n",
        "a = datahackdata.iloc[:,4]\n",
        "\n",
        "# datahackdata.head()\n",
        "# x=datahackdata.package_headline\n",
        "print(a)"
      ],
      "metadata": {
        "id": "f_ndl9idOEr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
